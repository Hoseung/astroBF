{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (truncated) GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Application of the Gaussian Mixture Model \n",
    "# to find the right threshould level for segmentation,  \n",
    "# finding the target in the central image of astronomical data, and\n",
    "# producing the mask for the target.\n",
    "# Developed by Min-Su Shin (msshin@kasi.re.kr)\n",
    "\n",
    "import sys, math\n",
    "\n",
    "from astropy.io import fits\n",
    "from sklearn import mixture\n",
    "#from skimage import measure\n",
    "#from skimage.morphology import convex_hull_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from astrobf.utils.gmm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters\n",
    "max_n_comp = 30\n",
    "max_iter_gmm = 300\n",
    "tol_gmm = 0.0001\n",
    "range_cut_min = 0.1\n",
    "range_cut_max = 95.0\n",
    "num_sample_x = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "fits_fn = \"../example_data/J000311.00+155754.0-i.fits\"\n",
    "hdulist = fits.open(fits_fn)\n",
    "img_header = hdulist[0].header\n",
    "img_data = hdulist[0].data\n",
    "hdulist.close()\n",
    "width=img_data.shape[0]\n",
    "height=img_data.shape[1]\n",
    "img_data_1d = img_data.reshape(-1, 1)\n",
    "num_pixels = width * height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIC, AIC, AICC are GMM statistics measuring fit performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own GMM implementation\n",
    "from scipy.stats import norm\n",
    "\n",
    "class GMM1D():\n",
    "    \"\"\"\n",
    "    Following sklearn standards.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, max_iter = 100, comp_names=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        if comp_names == None:\n",
    "            self.comp_names = [f\"comp{index}\" for index in range(self.n_components)]\n",
    "        else:\n",
    "            self.comp_names = comp_names\n",
    "        \n",
    "        self.pi = [1/self.n_components for comp in range(self.n_components)]\n",
    "    \n",
    "    def multivariate_normal2(self, X, mean_vector, covariance_matrix):\n",
    "        return ((2*np.pi)**(len(X))*np.linalg.det(covariance_matrix))**(-1/2)\\\n",
    "                *(np.exp(-1/2*np.dot(np.dot((X-mean_vector).T, \n",
    "                                            np.linalg.inv(covariance_matrix)), \n",
    "                                     (X-mean_vector))))\n",
    "\n",
    "    def multivariate_normal(self, X, mean_vector, covariance_matrix):\n",
    "        return (2*np.pi)**(-len(X)/2)*np.linalg.det(covariance_matrix)**(-1/2)\\\n",
    "                    *np.exp(-np.dot(np.dot((X-mean_vector).T, \\\n",
    "                    np.linalg.inv(covariance_matrix)), (X-mean_vector))/2)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # split data in n_components\n",
    "        new_X = np.array_split(X, self.n_components)\n",
    "        # Initial computation of mean and covar\n",
    "        self.mean_vector = [np.mean(x, axis=0) for x in new_X]\n",
    "        self.covariance_matrices = [np.cov(x.T) for x in new_X]\n",
    "        del new_X\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Estimation step\n",
    "            # r : responsibility matrix\n",
    "            self.r = np.zeros((len(X), self.n_components)) # N_elements * N_class\n",
    "            for n in range(len(X)):\n",
    "                for k in range(self.n_components):\n",
    "                    print(X[n], self.mean_vector[k], self.covariance_matrices[k])\n",
    "                    self.r[n,k] = self.pi[k] * \\\n",
    "                        self.multivariate_normal(X[n], \n",
    "                                                 self.mean_vector[k],\n",
    "                                                 self.covariance_matrices[k])\n",
    "                    \n",
    "                    self.r[n,k] /= sum([self.pi[j]*self.multivariate_normal(X[n], self.mean_vector[j],\n",
    "                        self.covariance_matrices[j]) for j in range(self.n_components)])\n",
    "            N = np.sum(self.r, axis=0)\n",
    "            \n",
    "            # Maximazation step\n",
    "            self.mean_vector = np.zeros((self.n_components, len(X[0])))\n",
    "            # Update the mean vector\n",
    "            for k in range(self.n_components):\n",
    "                for n in range(len(X)):\n",
    "                    self.mean_vector[k] += self.r[n,k] * X[n]\n",
    "                    self.mean_vector = [1/N[k]*self.mean_vector[k] for k in range(self.n_components)]\n",
    "            # Initiate the list of the covariance matrices\n",
    "            self.covariance_matrices = [np.zeros((len(X[0]), len(X[0]))) for k in self.n_components]\n",
    "            # Update covariace matrices\n",
    "            for k in range(self.n_components):\n",
    "                self.covariance_matrices[k] = np.cov(X.T, aweights=(self.r[:,k]), ddof=0)\n",
    "            self.covariance_matrices = [1/N[k] * self.covariance_matrices[k] for k in range(self.n_components)]\n",
    "            # Update the pi list\n",
    "            self.pi = [N[k]/len(X) for k in range(self.n_components)]\n",
    "            \n",
    "        def predict(self, X):\n",
    "            probas=[]\n",
    "            for n in range(len(X)):\n",
    "                probas.append([self.multivariate_normal(X[n], self.mean_vector[k], \n",
    "                                   self.covariance_matrices[k]) for k in range(self.n_components)])\n",
    "            cluster = []\n",
    "            for proba in probas:\n",
    "                cluster.append(self.comp_names[proba.index(max(proba))])\n",
    "            return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... best_n_comp:  5  with criteria val:  -45242.8579908\n"
     ]
    }
   ],
   "source": [
    "# Criteria to determine the number of components.\n",
    "# Note that these criteria don't tell you what's the best model, just the number of components.\n",
    "\n",
    "bic_list = [] # Bayesian Information Critetion\n",
    "aic_list = [] # Akaike Information Criterion\n",
    "aicc_list = [] # corrected Akaike Information Criterion\n",
    "model_list = []\n",
    "for n_comp in range(1, max_n_comp+1):\n",
    "    gmm = mixture.GaussianMixture(n_components = n_comp, \n",
    "         covariance_type = 'full', tol = tol_gmm, max_iter = max_iter_gmm)\n",
    "    #gmm = GMM(n_components = n_comp, max_iter = max_iter_gmm)\n",
    "    model = gmm.fit(img_data_1d)\n",
    "    model_list.append(model)\n",
    "    bic_list.append(gmm.bic(img_data_1d))\n",
    "    aic = gmm.aic(img_data_1d)\n",
    "    aic_list.append(aic)\n",
    "    aicc_list.append(gmm_aicc(aic, gmm._n_parameters(), num_pixels))\n",
    "\n",
    "plot_gmm_statstics(max_n_comp, bic_list, aic_list, aicc_list)\n",
    "\n",
    "best_n_comp, best_val = get_best_gmm(aic_list)\n",
    "\n",
    "print(\"... best_n_comp: \", best_n_comp, \" with criteria val: \", best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the best model\n",
    "best_model = model_list[best_n_comp-1]\n",
    "percent_values = np.percentile(img_data, [range_cut_min, range_cut_max])\n",
    "test_x = np.linspace(percent_values[0], percent_values[1], num_sample_x)\n",
    "logprob = best_model.score_samples(test_x.reshape(-1, 1))\n",
    "responsibilities = best_model.predict_proba(test_x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "if not best_model.converged_ :\n",
    "    print(\"[PROBLEM] ... however, not converged.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "pdf_comp_weights = best_model.weights_\n",
    "pdf_comp_means = best_model.means_\n",
    "pdf_comp_covariances = best_model.covariances_\n",
    "dominant_comp_ind = np.argmax(pdf_comp_weights)\n",
    "use_mean = pdf_comp_means[dominant_comp_ind].flatten()[0]\n",
    "use_std = math.sqrt(pdf_comp_covariances[dominant_comp_ind].flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... num_labels:  1089\n",
      "... best_label: 236 with num_label_region: 7437\n",
      "... use_ind.size:  7437\n",
      "... num_labels:  369\n",
      "... best_label: 123 with num_label_region: 5644\n",
      "... use_ind.size:  5644\n",
      "... num_labels:  103\n",
      "... best_label: 24 with num_label_region: 4726\n",
      "... use_ind.size:  4726\n",
      "... num_labels:  63\n",
      "... best_label: 10 with num_label_region: 3928\n",
      "... use_ind.size:  3928\n",
      "... num_labels:  52\n",
      "... best_label: 3 with num_label_region: 3310\n",
      "... use_ind.size:  3310\n"
     ]
    }
   ],
   "source": [
    "# sigma cut\n",
    "use_factors = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "for use_factor in use_factors:\n",
    "    cut_val = use_mean + use_factor*use_std\n",
    "    binary_result = img_data > cut_val\n",
    "    use_label, num_labels = measure.label(binary_result, background=0, return_num=True)\n",
    "    min_label = np.min(use_label)\n",
    "    max_label = np.max(use_label)\n",
    "    print(\"... num_labels: \", num_labels)\n",
    "    \n",
    "    # find the mean x, y for each label component\n",
    "    num_label_region_dict = dict()\n",
    "    mean_x_list = []\n",
    "    mean_y_list = []\n",
    "    mean_xy_distance_ratio_list = []\n",
    "    \n",
    "    for ind in range(1, max_label+1):\n",
    "        selected_region_ind = np.argwhere(use_label == ind)\n",
    "        num_label_region_dict[ind] = selected_region_ind.shape[0]\n",
    "        if selected_region_ind.shape[0] == 1:\n",
    "            mean_y, mean_x = selected_region_ind[0]\n",
    "            #mean_y = selected_region_ind[0][0]\n",
    "        else:    \n",
    "            # [WARNING] because of pyplot image show convention and data indexing scheme,\n",
    "            # x and y index should be used with caution.\n",
    "            mean_y, mean_x = np.sum(selected_region_ind, axis=0)/selected_region_ind.shape[0]\n",
    "        mean_x_list.append(mean_x)\n",
    "        mean_y_list.append(mean_y)\n",
    "        mean_xy_distance_ratio_list.append((mean_x/width - 0.5)**2 + (mean_y/height - 0.5)**2)\n",
    "    \n",
    "    # find the central object and its label\n",
    "    best_label, best_ind = get_central_label(mean_xy_distance_ratio_list, max_label)\n",
    "    print(\"... best_label: %d with num_label_region: %d\" % \\\n",
    "    (best_label, num_label_region_dict[best_label]))\n",
    "    # target mask\n",
    "    target_mask = np.zeros(img_data.shape, dtype=bool)    \n",
    "    use_ind = np.where(use_label == best_label)\n",
    "    print(\"... use_ind.size: \", len(use_ind[0]))\n",
    "    target_mask = np.zeros(img_data.shape, dtype=bool)\n",
    "    target_mask[use_ind] = True\n",
    "\n",
    "#    # [SKIP] contours\n",
    "#    target_contours = measure.find_contours(img_data, level=cut_val, \\\n",
    "#    fully_connected='high', positive_orientation='high')\n",
    "#    print(\"... len(target_contours): \", len(target_contours))\n",
    "    # convex hull\n",
    "    convex_hull_results = convex_hull_image(target_mask, offset_coordinates=False, tolerance=1e-20)\n",
    "    plot_mixture_cut(img_data, use_label, \n",
    "                     convex_hull_results, binary_result, \n",
    "                     mean_x_list, mean_y_list, best_ind, \n",
    "                     use_factor=use_factor,\n",
    "                     vmin=percent_values[0], vmax=percent_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
